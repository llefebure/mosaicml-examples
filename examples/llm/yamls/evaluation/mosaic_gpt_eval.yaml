

tokenizer:
  type: hftokenizer
  args:
    tokenizer_name: gpt2
    max_seq_len: 2048

model: 
  model_type: mosaic_gpt
  checkpoint: s3://mosaicml-internal-checkpoints-shared/nlp/mosaicgpt/mosaicgpt-1b-TaT8te/checkpoints/ep0-ba24000-rank0.pt
  config: yamls/mosaic_gpt/1b.yaml
  
icl_tasks:
  -
    label: hellaswag
    dataset_uri: s3://mosaicml-internal-dataset-hellaswag/hellaswag.jsonz
    num_fewshot: 
     - 5
    batch_size: 16
    type: multiple_choice
    metrics:
      - InContextLearningMultipleChoiceAccuracy
    formatting_options:
      prompt_string: ""
      example_delimiter: "\n"
      continuation_delimiter: " "  
  -
    label: piqa
    dataset_uri: s3://mosaicml-internal-dataset-hellaswag/piqa.jsonz
    num_fewshot: 
     - 5
    batch_size: 16
    type: multiple_choice
    metrics:
      - InContextLearningMultipleChoiceAccuracy
    formatting_options:
      prompt_string: ""
      example_delimiter: "\n"
      continuation_delimiter: " "  
  -
    label: lambada
    dataset_uri: s3://mosaicml-internal-dataset-lambda/lambada/lambada_test.json
    num_fewshot: 
     - 0
     - 1
    batch_size: 16
    type: language_modeling
    metrics:
      - InContextLearningLMAccuracy
    formatting_options:
      prompt_string: ""
      example_delimiter: "\n"
      continuation_delimiter: ""  