

# Tokenizer
tokenizer:
  type: hftokenizer
  args:
    tokenizer_name: gpt2
    max_seq_len: 2048

model: 
  model_type: pretrained_hf
  config: EleutherAI/gpt-neo-1.3B

icl_tasks:
  -
    label: lambada
    dataset_uri: s3://mosaicml-internal-dataset-lambda/lambada/lambada_test.json
    num_fewshot: 
     - 0
    batch_size: 16
    type: language_modeling
    metrics:
      - InContextLearningLMAccuracy
    formatting_options:
      prompt_string: ""
      example_delimiter: "\n"
      continuation_delimiter: ""  